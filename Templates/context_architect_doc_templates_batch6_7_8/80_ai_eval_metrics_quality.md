# 80 – AI Evaluation Metrics & Quality Criteria

> How we measure whether the AI outputs are “good enough” for each task.
> Audience: AI engineers, product, QA, evaluation tooling.

## 1. Task-Based Metrics

- **For content analysis (extraction):**
  - {{ai_eval_extraction_metrics}}
- **For document generation (specs, docs):**
  - {{ai_eval_docs_metrics}}
- **For code generation:**
  - {{ai_eval_code_metrics}}

## 2. Human Review Guidelines

- **What reviewers should check and how they score things:**
  - {{ai_eval_human_review_guidelines}}

## 3. Thresholds

- **Minimum acceptable quality thresholds per task/tier:**
  - {{ai_eval_thresholds}}

## 4. Feedback Loop

- **How feedback from users/reviewers feeds back into prompts/routing:**
  - {{ai_eval_feedback_loop}}

## 5. Notes for AI Tools

- Use these metrics when self-checking outputs or deciding when to ask humans.
